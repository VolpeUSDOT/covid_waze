skip = 1,
col_types = "iciiiiiiiiiiii",
na = c("...")
) %>%
select(-Type) %>%
gather(Date, Value, -Year) %>%
unite("Date", c(Date, Year), sep = " ") %>%
mutate(
Date = Date %>%
lubridate::parse_date_time("m y") %>%
yearmonth()
) %>%
drop_na() %>%
as_tsibble(index = "Date") %>%
filter(Date <= "2018-12-01")
??yearmonth
# load the packages
suppressPackageStartupMessages(require(tidyverse))
suppressPackageStartupMessages(require(tsibble))
suppressPackageStartupMessages(require(randomForest))
suppressPackageStartupMessages(require(forecast))
library(lubridate)
# specify the csv file (your path here)
file <- "~/Temp_Working_Docs/tax.csv"# TS_Data_Example.csv"
# read in the csv file
tax_tbl <- readr::read_delim(
file = file,
delim = ";",
col_names = c("Year", "Type", month.abb),
skip = 1,
col_types = "iciiiiiiiiiiii",
na = c("...")
) %>%
select(-Type) %>%
gather(Date, Value, -Year) %>%
unite("Date", c(Date, Year), sep = " ") %>%
mutate(
Date = Date %>%
lubridate::parse_date_time("m y") %>%
yearmonth()
) %>%
drop_na() %>%
as_tsibble(index = "Date") %>%
filter(Date <= "2018-12-01")
# convert to ts format
tax_ts <- as.ts(tax_tbl)
tax_ts
head(tax_tbl)
# implicit missings
has_gaps(tax_tbl)
# explicit missings
colSums(is.na(tax_tbl[, "Value"]))
# visualize
plot_org <- tax_tbl %>%
ggplot(aes(Date, Value / 1000)) + # to get the axis on a more manageable scale
geom_line() +
theme_minimal() +
labs(title = "German Wage and Income Taxes 1999 - 2018", x = "Year", y = "Euros")
plot_org
# pretend we're in December 2017 and have to forecast the next twelve months
tax_ts_org <- window(tax_ts, end = c(2017, 12))
# estimate the required order of differencing
n_diffs <- nsdiffs(tax_ts_org)
n_diffs
?nsdiffs
# log transform and difference the data
tax_ts_trf <- tax_ts_org %>%
log() %>%
diff(n_diffs)
tax_ts_trf
# check out the difference! (pun)
plot_trf <- tax_ts_trf %>%
autoplot() +
xlab("Year") +
ylab("Euros") +
ggtitle("German Wage and Income Taxes 1999 - 2018") +
theme_minimal()
gridExtra::grid.arrange(plot_org, plot_trf)
lag_order <- 6 # the desired number of lags (six months)
horizon <- 12 # the forecast horizon (twelve months)
tax_ts_mbd <- embed(tax_ts_trf, lag_order + 1) # embedding magic!
tax_ts_mbd
?embed
tax_ts_trf
tax_ts_trf
tax_ts_trf[2,1] # Feb 1999 value
tax_ts_trf[2,1] # Feb 1999 value
tax_ts_trf[1,2] # Feb 1999 value
tax_ts_trf[1,] # Feb 1999 value
dim(tax_ts_trf[1,])
dim(tax_ts_trf)
str(tax_ts_trf)
dim(tax_ts_trf[1])
dim(tax_ts_trf[2])
tax_ts_trf[2]
tax_ts_trf
tax_ts_trf[2]
tax_ts_trf
str(tax_ts_trf)
tax_ts_trf[1] # Feb 1999 value
tax_ts_mbd
tax_ts_mbd[1,]
tax_ts_trf[1:7] # Feb 1999 value
tax_ts_mbd[1,]
tax_ts_trf[1:7] # Feb 1999 to Aug 1999
tax_ts_mbd[1,]
tax_ts_mbd[2,]
y_train <- tax_ts_mbd[, 1] # the target
y_train
tax_ts_mbd[, 1]
tax_ts_mbd
y_test <- window(tax_ts, start = c(2018, 1), end = c(2018, 12)) # the year 2018
y_test
X_test <- tax_ts_mbd[nrow(tax_ts_mbd), c(1:lag_order)] # the test set consisting
X_test
forecasts_rf <- numeric(horizon)
forecasts_rf
horizon
for (i in 1:horizon){
# set seed
set.seed(2019)
# fit the model
fit_rf <- randomForest(X_train, y_train)
# predict using the test set
forecasts_rf[i] <- predict(fit_rf, X_test)
# here is where we repeatedly reshape the training data to reflect the time distance
# corresponding to the current forecast horizon.
y_train <- y_train[-1]
X_train <- X_train[-nrow(X_train), ]
}
y_train <- tax_ts_mbd[, 1] # the target (First column of the data, e.g. starting with August 1999)
X_train <- tax_ts_mbd[, -1] # everything but the target (Second to last columns of the data)
y_test <- window(tax_ts, start = c(2018, 1), end = c(2018, 12)) # the year 2018
X_test <- tax_ts_mbd[nrow(tax_ts_mbd), c(1:lag_order)] # the test set consisting
forecasts_rf <- numeric(horizon)
for (i in 1:horizon){
# set seed
set.seed(2019)
# fit the model
fit_rf <- randomForest(X_train, y_train)
# predict using the test set
forecasts_rf[i] <- predict(fit_rf, X_test)
# here is where we repeatedly reshape the training data to reflect the time distance
# corresponding to the current forecast horizon.
y_train <- y_train[-1]
X_train <- X_train[-nrow(X_train), ]
}
# calculate the exp term
exp_term <- exp(cumsum(forecasts_rf))
forecasts_rf
# extract the last observation from the time series (y_t)
last_observation <- as.vector(tail(tax_ts_org, 1))
exp_term
forecasts_rf
# calculate the exp term
exp_term <- exp(cumsum(forecasts_rf))
# extract the last observation from the time series (y_t)
last_observation <- as.vector(tail(tax_ts_org, 1))
# calculate the final predictions
backtransformed_forecasts <- last_observation * exp_term
# convert to ts format
y_pred <- ts(
backtransformed_forecasts,
start = c(2018, 1),
frequency = 12
)
# add the forecasts to the original tibble
tax_tbl <- tax_tbl %>%
mutate(Forecast = c(rep(NA, length(tax_ts_org)), y_pred))
# visualize the forecasts
plot_fc <- tax_tbl %>%
ggplot(aes(x = Date)) +
geom_line(aes(y = Value / 1000)) +
geom_line(aes(y = Forecast / 1000), color = "blue") +
theme_minimal() +
labs(
title = "Forecast of the German Wage and Income Tax for the Year 2018",
x = "Year",
y = "Euros"
)
accuracy(y_pred, y_test)
?accuracy
y_pred
plot(y_pred)
lines(y_test)
benchmark <- forecast(snaive(tax_ts_org), h = horizon)
tax_ts %>%
autoplot() +
autolayer(benchmark, PI = FALSE)
accuracy(benchmark, y_test)
snaive
?snaive
# load the packages
library(tidyverse)
library(tsibble)
library(randomForest)
library(forecast)
# Data
df <- read_csv('Output/2020-05-26/Waze_Covid_joined.csv')
head(df)
auto_export_bucket = 's3://prod-sdc-waze-autoexport-911061262852-us-east-1-bucket/alert/'
code_loc = '~/git/covid_waze'
local_dir = file.path(code_loc, 'Output', Sys.Date())
if (!dir.exists(local_dir)) {
dir.create(local_dir)
}
# Contintue to paste this into Anaconda Prompt
writeClipboard(
paste0('python ',
file.path(path.expand(code_loc), 'utility', 'DF_samlapi_formauth_adfs3_windows.py'))
)
system(paste0('aws --profile sdc s3 ls ', auto_export_bucket))
paste0('aws --profile sdc s3 ls ', auto_export_bucket)
paste0('aws --profile sdc s3 ls ', auto_export_bucket, Sys.Date(), '/')
writeClipboard(
paste0('aws --profile sdc s3 cp ',
auto_export_bucket,
Sys.Date(), '/',
'Waze_2020_Index_cleaned.csv',
' ',
path.expand(local_dir), '/',
'Waze_2020_Index_cleaned.csv')
)
# Contintue to paste this into Anaconda Prompt
writeClipboard(
paste0('python ',
file.path(path.expand(code_loc), 'utility', 'DF_samlapi_formauth_adfs3_windows.py'))
)
system(paste0('aws --profile sdc s3 ls ', auto_export_bucket))
paste0('aws --profile sdc s3 ls ', auto_export_bucket)
paste0('aws --profile sdc s3 ls ', auto_export_bucket, Sys.Date(), '/')
writeClipboard(
paste0('aws --profile sdc s3 cp ',
auto_export_bucket,
Sys.Date(), '/',
'Waze_2020_Index_cleaned.csv',
' ',
path.expand(local_dir), '/',
'Waze_2020_Index_cleaned.csv')
)
# setup ----
library(tidyverse)
library(readr)
input.loc = 'Data'
output.loc = 'Output'
latest_refresh_day = max(dir('Output')[grep('2020-', dir('Output'))]) # e.g. '2020-05-06'
latest_refresh_day
d_full <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_Covid_joined.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
d <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_Index_cleaned.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
date_count <- d %>%
group_by(date) %>%
summarize(total_Waze_count = sum(count_ACCIDENT, count_JAM, count_WEATHERHAZARD, na.rm = T),
count_NA = sum(is.na(count_ACCIDENT), is.na(count_JAM), is.na(count_WEATHERHAZARD)))
date_count_full <- d_full %>%
group_by(date) %>%
summarize(total_Waze_count = sum(count, na.rm = T),
count_NA = sum(is.na(count)))
co_date_change
d = d[order(d$fips, d$date),]
d$change_crash = with(d, (count_ACCIDENT - dplyr::lag(count_ACCIDENT) ) / count_ACCIDENT)
d$change_jam = with(d, (count_JAM - dplyr::lag(count_JAM) ) / count_JAM)
d$change_weh = with(d, (count_WEATHERHAZARD - dplyr::lag(count_WEATHERHAZARD) ) / count_WEATHERHAZARD)
d$mean_change = rowSums(d[,c('change_crash', 'change_jam', 'change_weh')], na.rm = T)/3
# test
d %>% dplyr::select(fips, date, count_ACCIDENT, change_crash, count_JAM, change_jam, mean_change) %>%
filter(fips == '01097')
d_change = d %>%
ungroup() %>%
group_by(fips) %>%
summarize(n_daily_change_200 = sum(abs(mean_change) > 2),
n_daily_change_500 = sum(abs(mean_change) > 5),
n_daily_change_1000 = sum(abs(mean_change) > 10))
hist(d_change$n_daily_change_500)
n_counties_with_200_change = nrow(d_change %>% filter(n_daily_change_200 > 10))
n_counties_with_500_change = nrow(d_change %>% filter(n_daily_change_500 > 10))
n_counties_with_1000_change = nrow(d_change %>% filter(n_daily_change_1000 > 10))
n_counties_total = nrow(d_change)
cat(n_counties_with_200_change, 'of', n_counties_total, 'counties had more than 10 days with a daily total change in Waze events of at least +/- 200%')
cat(n_counties_with_500_change, 'of', n_counties_total, 'counties had more than 10 days with a daily total change in Waze events of at least +/- 500%')
cat(n_counties_with_1000_change, 'of', n_counties_total, 'counties had more than 10 days with a daily total change in Waze events of at least +/- 1000%')
co_NA_count = d %>%
group_by(fips) %>%
summarize(NA_crash_threshold = sum(is.na(bl2020_mean_ACCIDENT)),
NA_jam_threshold = sum(is.na(bl2020_mean_JAM)),
NA_weh_threshold = sum(is.na(bl2020_mean_WEATHERHAZARD)),
NA_crash_nonthreshold = sum(is.na(bl2020_mean_ACCIDENT_nf)),
NA_jam_nonthreshold = sum(is.na(bl2020_mean_JAM_nf)),
NA_weh_nonthreshold = sum(is.na(bl2020_mean_WEATHERHAZARD_nf))
)
# Test
d %>% filter(fips == '01005') %>% select(fips, date,
bl2020_mean_ACCIDENT, bl2020_mean_ACCIDENT_nf,
bl2020_mean_JAM, bl2020_mean_JAM_nf)
co_NA_count %>% filter(fips == '01005')
ggplot(co_NA_count, aes(x = NA_crash_threshold, y = NA_crash_nonthreshold)) +
geom_count() +
scale_size_area() +
geom_abline(slope = 1, intercept = 0, lty = 2)
ggplot(date_count, aes(x = date, y = total_Waze_count)) +
geom_line() +
ggtitle('Total Waze events by date')
ggplot(date_count, aes(x = date, y = count_NA)) +
geom_line() +
ggtitle('Total counties x alert types with NA Waze data by date')
ggplot(date_count, aes(x = date, y = count_NA)) +
geom_line() +
ggtitle('Total counties x alert types with NA Waze data by date')
ggplot(date_count, aes(x = date, y = total_Waze_count)) +
geom_line() +
ggtitle('Total Waze events by date')
summary(d)
tail(D)
tail(d)
ggplot(d %>% filter(state == 'MA'), aes(x = date, y = cases)) + geom_line()
ggplot(d %>% filter(state == 'MA'), aes(x = date, y = cases)) + geom_line() + facet_wrap(~fips)
writeClipboard(
paste0('aws --profile sdc s3 cp ',
auto_export_bucket,
Sys.Date(), '/',
'Waze_2020_Index_cleaned.csv',
' ',
path.expand(local_dir), '/',
'Waze_2020_Index_cleaned.csv')
)
# setup ----
library(tidyverse)
library(readr)
input.loc = 'Data'
output.loc = 'Output'
latest_refresh_day = max(dir('Output')[grep('2020-', dir('Output'))]) # e.g. '2020-05-06'
d_full <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_Covid_joined.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
d <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_Index_cleaned.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
date_count <- d %>%
group_by(date) %>%
summarize(total_Waze_count = sum(count_ACCIDENT, count_JAM, count_WEATHERHAZARD, na.rm = T),
count_NA = sum(is.na(count_ACCIDENT), is.na(count_JAM), is.na(count_WEATHERHAZARD)))
date_count_full <- d_full %>%
group_by(date) %>%
summarize(total_Waze_count = sum(count, na.rm = T),
count_NA = sum(is.na(count)))
ggplot(d %>% filter(state == 'MA'), aes(x = date, y = cases)) + geom_line() + facet_wrap(~fips)
# setup ----
library(tidyverse)
library(readr)
input.loc = 'Data'
output.loc = 'Output'
latest_refresh_day = max(dir('Output')[grep('2020-', dir('Output'))]) # e.g. '2020-05-06'
latest_refresh_day
d <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_Index_cleaned.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
ggplot(d %>% filter(state == 'MA'), aes(x = date, y = cases)) + geom_line() + facet_wrap(~fips)
install.packages(c("egg", "usmap"))
install.packages("reticulate")
library(reticulate) # help set up python environment in R
if(!dir.exists(
file.path(dirname(path.expand('~/')),
'AppData', 'Local', 'r-miniconda'))) {
conda_create('r-reticulate')
conda_install('requests')
conda_install(envname = 'r-reticulate', packages = c('requests', 'configparser'))
}
conda_install(envname = 'r-reticulate', packages = c('requests', 'configparser'))
use_virtualenv("r-reticulate")
file.path(dirname(path.expand('~/'))
)
file.path(dirname(path.expand('~/')),
'AppData', 'Local', 'r-miniconda')
py_config()
use_virtualenv("C:/Users/Jessie.Yang.CTR/Documents/.conda/envs/r-reticulate")
auto_export_bucket = 's3://prod-sdc-waze-autoexport-004118380849/alert/'
volpe_drive = '//vntscex.local/DFS/Projects/PROJ-OS62A1/SDI Waze Phase 2/Output/COVID'
code_loc = '~/git/covid_waze'
local_dir = file.path(code_loc, 'Output', Sys.Date())
local_dir = file.path('Output', Sys.Date())
local_dir
if (!dir.exists(local_dir)) {
dir.create(local_dir)
}
path.expand(local_dir)
if (!dir.exists(local_dir)) {
dir.create(local_dir, recursive = T)
#dir.create(local_dir)
}
if(!file.exists(file.path(path.expand(getwd()),
'utility',
'auto_export_waze.py'))){
stop('Contact sdc-support@dot.gov to set up auto-export permissions and be given the appropriate authentication script.')
}
# Refresh credentials
reticulate::source_python(file = file.path(path.expand(getwd()),
'utility',
'auto_export_waze.py'))
Sys.Date()
use_date = Sys.Date()  -1
local_dir = file.path('Output', use_date)
local_dir
if (!dir.exists(local_dir)) {
dir.create(local_dir, recursive = T)
}
if(!file.exists(file.path(path.expand(getwd()),
'utility',
'auto_export_waze.py'))){
stop('Contact sdc-support@dot.gov to set up auto-export permissions and be given the appropriate authentication script.')
}
system(
paste0('aws --profile sdc-token s3 ls ', auto_export_bucket, use_date, '/'))
get_files = c('Waze_2020_MSA_day.csv',
'Waze_2020_MSA_week.csv',
'Waze_2020_National_day.csv',
'Waze_2020_National_week.csv',
paste0('Waze_Covid_joined_', use_date, '.csv'))
for(file in get_files){
if(grepl(use_date, file)){
out_file = 'Waze_Full.csv'
} else {
out_file = file
}
system(
paste0('aws --profile sdc-token s3 cp ',
auto_export_bucket,
use_date, '/',
file,
' ',
path.expand(local_dir), '/',
out_file)
)
}
source('Analysis/Waze_Index_Calcs_Check.R')
# setup ----
library(tidyverse)
library(lubridate)
library(readr)
input.loc = 'Data'
output.loc = 'Output'
drive.output = '//vntscex.local/DFS/Projects/PROJ-OS62A1/SDI Waze Phase 2/Data/COVID'
latest_refresh_day = max(dir('Output')[grep(format(Sys.Date(), '%Y'), dir('Output'))]) # e.g. '2020-05-06'
latest_refresh_day
cat(latest_refresh_day)
nw <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_National_week.csv'))
nd <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_National_day.csv'))
# Daily MSA
d_MSA_day <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_MSA_day.csv'),
col_types = cols(dowavg19_ACCIDENT = col_double(),
dowavg19_JAM = col_double(),
dowavg19_WEATHERHAZARD = col_double(),
bl2020_mean_ACCIDENT = col_double(),
bl2020_mean_JAM = col_double(),
bl2020_mean_WEATHERHAZARD = col_double()
))
d_MSA_week <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_MSA_week.csv'),
col_types = cols(weeksum19_ACCIDENT = col_double(),
weeksum19_JAM = col_double(),
weeksum19_WEATHERHAZARD = col_double(),
bl2020_mean_ACCIDENT = col_double(),
bl2020_mean_JAM = col_double(),
bl2020_mean_WEATHERHAZARD = col_double()
))
nw <- nw %>%
ungroup %>%
group_by(year, week) %>%
mutate(WoY_Weight_Jams_19 = weeksum19_JAM_nf / sum(weeksum19_JAM_nf, na.rm=T),
WoY_Weight_Crash_19 = weeksum19_ACCIDENT_nf / sum(weeksum19_ACCIDENT_nf, na.rm=T),
WoY_Weight_Jams_bl = bl2020_mean_JAM_nf / sum(bl2020_mean_JAM_nf, na.rm=T),
WoY_Weight_Crash_bl = bl2020_mean_ACCIDENT_nf / sum(bl2020_mean_ACCIDENT_nf, na.rm=T),
WoY_Weight_Jams_lag1 = lag1_weeksum20_JAM_nf / sum(lag1_weeksum20_JAM_nf, na.rm=T),
WoY_Weight_Crash_lag1 = lag1_weeksum20_ACCIDENT_nf / sum(lag1_weeksum20_ACCIDENT_nf, na.rm=T)
) %>%
ungroup()
week_index_calcs <- nw %>%
ungroup() %>%
mutate(
pct_ch_from_prev_week_jam = 100 *  ( (weeksum20_JAM_nf - lag1_weeksum20_JAM_nf) / lag1_weeksum20_JAM_nf ),
pct_ch_from_prev_week_crash = 100 *  ( (weeksum20_ACCIDENT_nf - lag1_weeksum20_ACCIDENT_nf) / lag1_weeksum20_ACCIDENT_nf ),
pct_ch_from_2019_week_jam = 100 *  ( (weeksum20_JAM_nf - weeksum19_JAM_nf) / weeksum19_JAM_nf ),
pct_ch_from_2019_week_crash = 100 *  ( (weeksum20_ACCIDENT_nf - weeksum19_ACCIDENT_nf) / weeksum19_ACCIDENT_nf ),
pct_ch_from_2020bl_week_jam = 100 *  ( (weeksum20_JAM_nf - bl2020_mean_JAM_nf) / bl2020_mean_JAM_nf ),
pct_ch_from_2020bl_week_crash = 100 *  ( (weeksum20_ACCIDENT_nf - bl2020_mean_ACCIDENT_nf) / bl2020_mean_ACCIDENT_nf )
) %>%
group_by(year, week) %>%
summarize(
weekly_sum_20_jam = sum(weeksum20_JAM_nf, na.rm = T),
weekly_sum_19_jam = sum(weeksum19_JAM_nf, na.rm = T),
change_from_19_jam = 100 * (weekly_sum_20_jam - weekly_sum_19_jam) / weekly_sum_19_jam,
pct_ch_from_prev_week_jam = weighted.mean(pct_ch_from_prev_week_jam, w = WoY_Weight_Jams_lag1),
#pct_ch_from_prev_week_crash = weighted.mean(pct_ch_from_prev_week_crash, w = WoY_Weight_Crash_lag1),
pct_ch_from_2019_week_jam = weighted.mean(pct_ch_from_2019_week_jam, w = WoY_Weight_Jams_19),
#pct_ch_from_2019_week_crash = weighted.mean(pct_ch_from_2019_week_crash, w = WoY_Weight_Crash_19),
pct_ch_from_2020bl_week_jam = weighted.mean(pct_ch_from_2020bl_week_jam, w = WoY_Weight_Jams_bl),
#pct_ch_from_2020bl_week_crash = weighted.mean(pct_ch_from_2020bl_week_crash, w = WoY_Weight_Crash_bl),
)
write.csv(week_index_calcs, file = file.path(drive.output, 'Weekly_Covid_Outputs.csv'), row.names = F)
# look for leap years
days_in_year_20 = ifelse(lubridate::days_in_month('2020-02-01') == 29, 366, 365)
days_in_year_21 = ifelse(lubridate::days_in_month('2021-02-01') == 29, 366, 365)
days_in_year_22 = ifelse(lubridate::days_in_month('2022-02-01') == 29, 366, 365)
dates = c(paste('2020', formatC(1:days_in_year_20, width = 2, flag = '0'), sep = '-'),
paste('2021', formatC(1:days_in_year_21, width = 2, flag = '0'), sep = '-'),
paste('2022', formatC(1:days_in_year_21, width = 2, flag = '0'), sep = '-'))
dates = strptime(dates, '%Y-%j')
week = lubridate::epiweek(dates)
lubridate::days_in_month('2020-02-01')
install.packages("lubridate")
install.packages("lubridate")
