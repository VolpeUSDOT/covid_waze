wazeall_county_bl2020 <- waze_bl2020 %>%
group_by(fips, state, county) %>%
summarise(bl2020_cty_sum_all = round(sum(bl2020_mean, na.rm=TRUE),1),
bl2020_cty_n_all = n())
wazeall_state_bl2020 <- waze_bl2020 %>%
group_by(state) %>%
summarise(bl2020_st_sum_all = round(sum(bl2020_mean, na.rm=TRUE),1),
bl2020_st_n_all = n())
View(wazeall_county_bl2020)
View(wazeall_state_bl2020)
=9372/3
9372/3
waze_county_bl2020 <- waze_bl2020 %>%
group_by(alert_type, fips, state, county) %>%
summarise(bl2020_cty_sum = round(sum(bl2020_mean, na.rm=TRUE),1),
bl2020_cty_n = n())
write.csv(waze_county_bl2020, file = file.path(output.loc, "waze_county_bl2020.csv"), row.names=FALSE)
waze_state_bl2020 <- waze_bl2020 %>%
group_by(alert_type, state) %>%
summarise(bl2020_st_sum = round(sum(bl2020_mean, na.rm=TRUE),1),
bl2020_st_n = n())
write.csv(waze_state_bl2020, file = file.path(output.loc, "waze_state_bl2020.csv"), row.names=FALSE)
wazeall_county_bl2020 <- waze_bl2020 %>%
group_by(fips, state, county) %>%
summarise(bl2020_cty_sum_all = round(sum(bl2020_mean, na.rm=TRUE),1),
bl2020_cty_n_all = n())
write.csv(wazeall_county_bl2020, file = file.path(output.loc, "wazeall_county_bl2020.csv"), row.names=FALSE)
wazeall_state_bl2020 <- waze_bl2020 %>%
group_by(state) %>%
summarise(bl2020_st_sum_all = round(sum(bl2020_mean, na.rm=TRUE),1),
bl2020_st_n_all = n())
write.csv(wazeall_state_bl2020, file = file.path(output.loc, "wazeall_state_bl2020.csv"), row.names=FALSE)
(131*28)+(102*33)+(16*133)
library(reticulate)
use_virtualenv("r-reticulate")
auto_export_bucket = 's3://prod-sdc-waze-autoexport-911061262852-us-east-1-bucket/alert/'
code_loc = '~/git/covid_waze'
local_dir = file.path(code_loc, 'Output', Sys.Date())
if (!dir.exists(local_dir)) {
dir.create(local_dir)
}
# Contintue to paste this into Anaconda Prompt
writeClipboard(
paste0('python ',
file.path(path.expand(code_loc), 'utility', 'DF_samlapi_formauth_adfs3_windows.py'))
)
writeClipboard(
paste0('aws --profile sdc s3 cp ',
auto_export_bucket,
Sys.Date(), '/',
'Waze_2020_Index_cleaned.csv',
' ',
path.expand(local_dir), '/',
'Waze_2020_Index_cleaned.csv')
)
# Also get joined data for full data dashboard
# Assumed this script is run on same day as reresh script within SDC.
writeClipboard(
paste0('aws --profile sdc s3 cp ',
auto_export_bucket,
Sys.Date(), '/',
'Waze_Covid_joined_', Sys.Date(),'.csv',
' ',
path.expand(local_dir), '/',
'Waze_Covid_joined.csv')
)
# Time-series RF modeling example
# https://www.statworx.com/at/blog/time-series-forecasting-with-random-forest/
# load the packages
suppressPackageStartupMessages(require(tidyverse))
suppressPackageStartupMessages(require(tsibble))
suppressPackageStartupMessages(require(randomForest))
suppressPackageStartupMessages(require(forecast))
# specify the csv file (your path here)
file <- "~/Documents/Temp_Working_Docs/TS_Data_Example.csv"
install.packages("tsibble")
# specify the csv file (your path here)
file <- "~/Documents/Temp_Working_Docs/TS_Data_Example.csv"
# read in the csv file
tax_tbl <- readr::read_delim(
file = file,
delim = ";",
col_names = c("Year", "Type", month.abb),
skip = 1,
col_types = "iciiiiiiiiiiii",
na = c("...")
) %>%
select(-Type) %>%
gather(Date, Value, -Year) %>%
unite("Date", c(Date, Year), sep = " ") %>%
mutate(
Date = Date %>%
lubridate::parse_date_time("m y") %>%
yearmonth()
) %>%
drop_na() %>%
as_tsibble(index = "Date") %>%
filter(Date <= "2018-12-01")
dir('~/')
# specify the csv file (your path here)
file <- "~/Temp_Working_Docs/TS_Data_Example.csv"
# read in the csv file
tax_tbl <- readr::read_delim(
file = file,
delim = ";",
col_names = c("Year", "Type", month.abb),
skip = 1,
col_types = "iciiiiiiiiiiii",
na = c("...")
) %>%
select(-Type) %>%
gather(Date, Value, -Year) %>%
unite("Date", c(Date, Year), sep = " ") %>%
mutate(
Date = Date %>%
lubridate::parse_date_time("m y") %>%
yearmonth()
) %>%
drop_na() %>%
as_tsibble(index = "Date") %>%
filter(Date <= "2018-12-01")
library(lubridate)
# specify the csv file (your path here)
file <- "~/Temp_Working_Docs/TS_Data_Example.csv"
# read in the csv file
tax_tbl <- readr::read_delim(
file = file,
delim = ";",
col_names = c("Year", "Type", month.abb),
skip = 1,
col_types = "iciiiiiiiiiiii",
na = c("...")
) %>%
select(-Type) %>%
gather(Date, Value, -Year) %>%
unite("Date", c(Date, Year), sep = " ") %>%
mutate(
Date = Date %>%
lubridate::parse_date_time("m y") %>%
yearmonth()
) %>%
drop_na() %>%
as_tsibble(index = "Date") %>%
filter(Date <= "2018-12-01")
# specify the csv file (your path here)
file <- "~/Temp_Working_Docs/tax.csv"# TS_Data_Example.csv"
# read in the csv file
tax_tbl <- readr::read_delim(
file = file,
delim = ";",
col_names = c("Year", "Type", month.abb),
skip = 1,
col_types = "iciiiiiiiiiiii",
na = c("...")
) %>%
select(-Type) %>%
gather(Date, Value, -Year) %>%
unite("Date", c(Date, Year), sep = " ") %>%
mutate(
Date = Date %>%
lubridate::parse_date_time("m y") %>%
yearmonth()
) %>%
drop_na() %>%
as_tsibble(index = "Date") %>%
filter(Date <= "2018-12-01")
??yearmonth
# load the packages
suppressPackageStartupMessages(require(tidyverse))
suppressPackageStartupMessages(require(tsibble))
suppressPackageStartupMessages(require(randomForest))
suppressPackageStartupMessages(require(forecast))
library(lubridate)
# specify the csv file (your path here)
file <- "~/Temp_Working_Docs/tax.csv"# TS_Data_Example.csv"
# read in the csv file
tax_tbl <- readr::read_delim(
file = file,
delim = ";",
col_names = c("Year", "Type", month.abb),
skip = 1,
col_types = "iciiiiiiiiiiii",
na = c("...")
) %>%
select(-Type) %>%
gather(Date, Value, -Year) %>%
unite("Date", c(Date, Year), sep = " ") %>%
mutate(
Date = Date %>%
lubridate::parse_date_time("m y") %>%
yearmonth()
) %>%
drop_na() %>%
as_tsibble(index = "Date") %>%
filter(Date <= "2018-12-01")
# convert to ts format
tax_ts <- as.ts(tax_tbl)
tax_ts
head(tax_tbl)
# implicit missings
has_gaps(tax_tbl)
# explicit missings
colSums(is.na(tax_tbl[, "Value"]))
# visualize
plot_org <- tax_tbl %>%
ggplot(aes(Date, Value / 1000)) + # to get the axis on a more manageable scale
geom_line() +
theme_minimal() +
labs(title = "German Wage and Income Taxes 1999 - 2018", x = "Year", y = "Euros")
plot_org
# pretend we're in December 2017 and have to forecast the next twelve months
tax_ts_org <- window(tax_ts, end = c(2017, 12))
# estimate the required order of differencing
n_diffs <- nsdiffs(tax_ts_org)
n_diffs
?nsdiffs
# log transform and difference the data
tax_ts_trf <- tax_ts_org %>%
log() %>%
diff(n_diffs)
tax_ts_trf
# check out the difference! (pun)
plot_trf <- tax_ts_trf %>%
autoplot() +
xlab("Year") +
ylab("Euros") +
ggtitle("German Wage and Income Taxes 1999 - 2018") +
theme_minimal()
gridExtra::grid.arrange(plot_org, plot_trf)
lag_order <- 6 # the desired number of lags (six months)
horizon <- 12 # the forecast horizon (twelve months)
tax_ts_mbd <- embed(tax_ts_trf, lag_order + 1) # embedding magic!
tax_ts_mbd
?embed
tax_ts_trf
tax_ts_trf
tax_ts_trf[2,1] # Feb 1999 value
tax_ts_trf[2,1] # Feb 1999 value
tax_ts_trf[1,2] # Feb 1999 value
tax_ts_trf[1,] # Feb 1999 value
dim(tax_ts_trf[1,])
dim(tax_ts_trf)
str(tax_ts_trf)
dim(tax_ts_trf[1])
dim(tax_ts_trf[2])
tax_ts_trf[2]
tax_ts_trf
tax_ts_trf[2]
tax_ts_trf
str(tax_ts_trf)
tax_ts_trf[1] # Feb 1999 value
tax_ts_mbd
tax_ts_mbd[1,]
tax_ts_trf[1:7] # Feb 1999 value
tax_ts_mbd[1,]
tax_ts_trf[1:7] # Feb 1999 to Aug 1999
tax_ts_mbd[1,]
tax_ts_mbd[2,]
y_train <- tax_ts_mbd[, 1] # the target
y_train
tax_ts_mbd[, 1]
tax_ts_mbd
y_test <- window(tax_ts, start = c(2018, 1), end = c(2018, 12)) # the year 2018
y_test
X_test <- tax_ts_mbd[nrow(tax_ts_mbd), c(1:lag_order)] # the test set consisting
X_test
forecasts_rf <- numeric(horizon)
forecasts_rf
horizon
for (i in 1:horizon){
# set seed
set.seed(2019)
# fit the model
fit_rf <- randomForest(X_train, y_train)
# predict using the test set
forecasts_rf[i] <- predict(fit_rf, X_test)
# here is where we repeatedly reshape the training data to reflect the time distance
# corresponding to the current forecast horizon.
y_train <- y_train[-1]
X_train <- X_train[-nrow(X_train), ]
}
y_train <- tax_ts_mbd[, 1] # the target (First column of the data, e.g. starting with August 1999)
X_train <- tax_ts_mbd[, -1] # everything but the target (Second to last columns of the data)
y_test <- window(tax_ts, start = c(2018, 1), end = c(2018, 12)) # the year 2018
X_test <- tax_ts_mbd[nrow(tax_ts_mbd), c(1:lag_order)] # the test set consisting
forecasts_rf <- numeric(horizon)
for (i in 1:horizon){
# set seed
set.seed(2019)
# fit the model
fit_rf <- randomForest(X_train, y_train)
# predict using the test set
forecasts_rf[i] <- predict(fit_rf, X_test)
# here is where we repeatedly reshape the training data to reflect the time distance
# corresponding to the current forecast horizon.
y_train <- y_train[-1]
X_train <- X_train[-nrow(X_train), ]
}
# calculate the exp term
exp_term <- exp(cumsum(forecasts_rf))
forecasts_rf
# extract the last observation from the time series (y_t)
last_observation <- as.vector(tail(tax_ts_org, 1))
exp_term
forecasts_rf
# calculate the exp term
exp_term <- exp(cumsum(forecasts_rf))
# extract the last observation from the time series (y_t)
last_observation <- as.vector(tail(tax_ts_org, 1))
# calculate the final predictions
backtransformed_forecasts <- last_observation * exp_term
# convert to ts format
y_pred <- ts(
backtransformed_forecasts,
start = c(2018, 1),
frequency = 12
)
# add the forecasts to the original tibble
tax_tbl <- tax_tbl %>%
mutate(Forecast = c(rep(NA, length(tax_ts_org)), y_pred))
# visualize the forecasts
plot_fc <- tax_tbl %>%
ggplot(aes(x = Date)) +
geom_line(aes(y = Value / 1000)) +
geom_line(aes(y = Forecast / 1000), color = "blue") +
theme_minimal() +
labs(
title = "Forecast of the German Wage and Income Tax for the Year 2018",
x = "Year",
y = "Euros"
)
accuracy(y_pred, y_test)
?accuracy
y_pred
plot(y_pred)
lines(y_test)
benchmark <- forecast(snaive(tax_ts_org), h = horizon)
tax_ts %>%
autoplot() +
autolayer(benchmark, PI = FALSE)
accuracy(benchmark, y_test)
snaive
?snaive
# load the packages
library(tidyverse)
library(tsibble)
library(randomForest)
library(forecast)
# Data
df <- read_csv('Output/2020-05-26/Waze_Covid_joined.csv')
head(df)
auto_export_bucket = 's3://prod-sdc-waze-autoexport-911061262852-us-east-1-bucket/alert/'
code_loc = '~/git/covid_waze'
local_dir = file.path(code_loc, 'Output', Sys.Date())
if (!dir.exists(local_dir)) {
dir.create(local_dir)
}
# Contintue to paste this into Anaconda Prompt
writeClipboard(
paste0('python ',
file.path(path.expand(code_loc), 'utility', 'DF_samlapi_formauth_adfs3_windows.py'))
)
system(paste0('aws --profile sdc s3 ls ', auto_export_bucket))
paste0('aws --profile sdc s3 ls ', auto_export_bucket)
paste0('aws --profile sdc s3 ls ', auto_export_bucket, Sys.Date(), '/')
writeClipboard(
paste0('aws --profile sdc s3 cp ',
auto_export_bucket,
Sys.Date(), '/',
'Waze_2020_Index_cleaned.csv',
' ',
path.expand(local_dir), '/',
'Waze_2020_Index_cleaned.csv')
)
# Contintue to paste this into Anaconda Prompt
writeClipboard(
paste0('python ',
file.path(path.expand(code_loc), 'utility', 'DF_samlapi_formauth_adfs3_windows.py'))
)
system(paste0('aws --profile sdc s3 ls ', auto_export_bucket))
paste0('aws --profile sdc s3 ls ', auto_export_bucket)
paste0('aws --profile sdc s3 ls ', auto_export_bucket, Sys.Date(), '/')
writeClipboard(
paste0('aws --profile sdc s3 cp ',
auto_export_bucket,
Sys.Date(), '/',
'Waze_2020_Index_cleaned.csv',
' ',
path.expand(local_dir), '/',
'Waze_2020_Index_cleaned.csv')
)
# setup ----
library(tidyverse)
library(readr)
input.loc = 'Data'
output.loc = 'Output'
latest_refresh_day = max(dir('Output')[grep('2020-', dir('Output'))]) # e.g. '2020-05-06'
latest_refresh_day
d_full <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_Covid_joined.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
d <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_Index_cleaned.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
date_count <- d %>%
group_by(date) %>%
summarize(total_Waze_count = sum(count_ACCIDENT, count_JAM, count_WEATHERHAZARD, na.rm = T),
count_NA = sum(is.na(count_ACCIDENT), is.na(count_JAM), is.na(count_WEATHERHAZARD)))
date_count_full <- d_full %>%
group_by(date) %>%
summarize(total_Waze_count = sum(count, na.rm = T),
count_NA = sum(is.na(count)))
co_date_change
d = d[order(d$fips, d$date),]
d$change_crash = with(d, (count_ACCIDENT - dplyr::lag(count_ACCIDENT) ) / count_ACCIDENT)
d$change_jam = with(d, (count_JAM - dplyr::lag(count_JAM) ) / count_JAM)
d$change_weh = with(d, (count_WEATHERHAZARD - dplyr::lag(count_WEATHERHAZARD) ) / count_WEATHERHAZARD)
d$mean_change = rowSums(d[,c('change_crash', 'change_jam', 'change_weh')], na.rm = T)/3
# test
d %>% dplyr::select(fips, date, count_ACCIDENT, change_crash, count_JAM, change_jam, mean_change) %>%
filter(fips == '01097')
d_change = d %>%
ungroup() %>%
group_by(fips) %>%
summarize(n_daily_change_200 = sum(abs(mean_change) > 2),
n_daily_change_500 = sum(abs(mean_change) > 5),
n_daily_change_1000 = sum(abs(mean_change) > 10))
hist(d_change$n_daily_change_500)
n_counties_with_200_change = nrow(d_change %>% filter(n_daily_change_200 > 10))
n_counties_with_500_change = nrow(d_change %>% filter(n_daily_change_500 > 10))
n_counties_with_1000_change = nrow(d_change %>% filter(n_daily_change_1000 > 10))
n_counties_total = nrow(d_change)
cat(n_counties_with_200_change, 'of', n_counties_total, 'counties had more than 10 days with a daily total change in Waze events of at least +/- 200%')
cat(n_counties_with_500_change, 'of', n_counties_total, 'counties had more than 10 days with a daily total change in Waze events of at least +/- 500%')
cat(n_counties_with_1000_change, 'of', n_counties_total, 'counties had more than 10 days with a daily total change in Waze events of at least +/- 1000%')
co_NA_count = d %>%
group_by(fips) %>%
summarize(NA_crash_threshold = sum(is.na(bl2020_mean_ACCIDENT)),
NA_jam_threshold = sum(is.na(bl2020_mean_JAM)),
NA_weh_threshold = sum(is.na(bl2020_mean_WEATHERHAZARD)),
NA_crash_nonthreshold = sum(is.na(bl2020_mean_ACCIDENT_nf)),
NA_jam_nonthreshold = sum(is.na(bl2020_mean_JAM_nf)),
NA_weh_nonthreshold = sum(is.na(bl2020_mean_WEATHERHAZARD_nf))
)
# Test
d %>% filter(fips == '01005') %>% select(fips, date,
bl2020_mean_ACCIDENT, bl2020_mean_ACCIDENT_nf,
bl2020_mean_JAM, bl2020_mean_JAM_nf)
co_NA_count %>% filter(fips == '01005')
ggplot(co_NA_count, aes(x = NA_crash_threshold, y = NA_crash_nonthreshold)) +
geom_count() +
scale_size_area() +
geom_abline(slope = 1, intercept = 0, lty = 2)
ggplot(date_count, aes(x = date, y = total_Waze_count)) +
geom_line() +
ggtitle('Total Waze events by date')
ggplot(date_count, aes(x = date, y = count_NA)) +
geom_line() +
ggtitle('Total counties x alert types with NA Waze data by date')
ggplot(date_count, aes(x = date, y = count_NA)) +
geom_line() +
ggtitle('Total counties x alert types with NA Waze data by date')
ggplot(date_count, aes(x = date, y = total_Waze_count)) +
geom_line() +
ggtitle('Total Waze events by date')
summary(d)
tail(D)
tail(d)
ggplot(d %>% filter(state == 'MA'), aes(x = date, y = cases)) + geom_line()
ggplot(d %>% filter(state == 'MA'), aes(x = date, y = cases)) + geom_line() + facet_wrap(~fips)
writeClipboard(
paste0('aws --profile sdc s3 cp ',
auto_export_bucket,
Sys.Date(), '/',
'Waze_2020_Index_cleaned.csv',
' ',
path.expand(local_dir), '/',
'Waze_2020_Index_cleaned.csv')
)
# setup ----
library(tidyverse)
library(readr)
input.loc = 'Data'
output.loc = 'Output'
latest_refresh_day = max(dir('Output')[grep('2020-', dir('Output'))]) # e.g. '2020-05-06'
d_full <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_Covid_joined.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
d <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_Index_cleaned.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
date_count <- d %>%
group_by(date) %>%
summarize(total_Waze_count = sum(count_ACCIDENT, count_JAM, count_WEATHERHAZARD, na.rm = T),
count_NA = sum(is.na(count_ACCIDENT), is.na(count_JAM), is.na(count_WEATHERHAZARD)))
date_count_full <- d_full %>%
group_by(date) %>%
summarize(total_Waze_count = sum(count, na.rm = T),
count_NA = sum(is.na(count)))
ggplot(d %>% filter(state == 'MA'), aes(x = date, y = cases)) + geom_line() + facet_wrap(~fips)
# setup ----
library(tidyverse)
library(readr)
input.loc = 'Data'
output.loc = 'Output'
latest_refresh_day = max(dir('Output')[grep('2020-', dir('Output'))]) # e.g. '2020-05-06'
latest_refresh_day
d <- read_csv(file.path(output.loc, latest_refresh_day, 'Waze_2020_Index_cleaned.csv'),
col_types = cols(cases = col_double(),
deaths = col_double()))
ggplot(d %>% filter(state == 'MA'), aes(x = date, y = cases)) + geom_line() + facet_wrap(~fips)
install.packages(c("egg", "usmap"))
